{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 8.])\n"
     ]
    }
   ],
   "source": [
    "#define tensors in pytorch - same as arrays in numpy - with tensor\n",
    "X = torch.tensor(([2,9], [1,5], [3,6]), dtype=torch.float) #3x2 tensor\n",
    "y = torch.tensor(([92], [100], [89]), dtype=torch.float) #3 by 1 tensor\n",
    "x_test = torch.tensor(([4, 8]), dtype=torch.float)\n",
    "\n",
    "#can check the size with .size()\n",
    "X.size()\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6667, 1.0000],\n",
      "        [0.3333, 0.5556],\n",
      "        [1.0000, 0.6667]])\n",
      "tensor([0.5000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "#apply scaling\n",
    "#max function returns max element and indice\n",
    "X_max, _ = torch.max(X, 0)\n",
    "xPredicted_max, _ = torch.max(x_test, 0)\n",
    "\n",
    "#divides all the elements by X_max - in this case X_max return largest single value in the matrices i.e. 9\n",
    "X = torch.div(X, X_max)\n",
    "x_test = torch.div(x_test, xPredicted_max)\n",
    "y = y/100\n",
    "print(X)\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building The Neural Network\n",
    "#nn.module is the base class for any module in PyTorch\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.inputSize = 2\n",
    "        self.outputSize = 1\n",
    "        self.hiddenNodes = 3\n",
    "        \n",
    "        self.W1 = torch.randn(self.inputSize, self.hiddenNodes) #Tensor is 2 by 3 since (1 by 2) * (2 by 3) gives desired (1 by 3)\n",
    "        self.W2 = torch.randn(self.hiddenNodes, self.outputSize) #Tesnsor is 3 by 1\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.z = torch.matmul(X, self.W1)\n",
    "        self.z2 = self.sigmoid(self.z)\n",
    "        self.z3 = torch.matmul(self.z2, self.W2)\n",
    "        o = self.sigmoid(self.z3)\n",
    "        return o\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1-s)\n",
    "    \n",
    "    def backward(self, x, y, o):\n",
    "        self.o_error = y - o\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        #torch.t is the transpose operation\n",
    "        #recall in the second wave of operations we do sigmoid(z2 * W2)\n",
    "        #thus this can be represented as two nodes in a computation graph, the sigmoid and the matrix multiplication\n",
    "        #partial derivative at the matrix multiplication level\n",
    "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
    "        #partial derivative at the sigmoid level\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
    "        self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
    "        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)\n",
    "    \n",
    "         \n",
    "    def train(self, X, y):\n",
    "        # forward + backward pass for training\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "        \n",
    "    def saveWeights(self, model):\n",
    "        # we will use the PyTorch internal storage functions\n",
    "        torch.save(model, \"NN\")\n",
    "        # you can reload model with all the weights and so forth with:\n",
    "        # torch.load(\"NN\")\n",
    "        \n",
    "    def predict(self):\n",
    "        print (\"Predicted data based on trained weights: \")\n",
    "        print (\"Input (scaled): \\n\" + str(x_test))\n",
    "        print (\"Output: \\n\" + str(self.forward(x_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Loss: 0.08693492412567139\n",
      "#1 Loss: 0.07286923378705978\n",
      "#2 Loss: 0.06154783442616463\n",
      "#3 Loss: 0.052441343665122986\n",
      "#4 Loss: 0.045097678899765015\n",
      "#5 Loss: 0.03914613649249077\n",
      "#6 Loss: 0.034290965646505356\n",
      "#7 Loss: 0.030300229787826538\n",
      "#8 Loss: 0.026993749663233757\n",
      "#9 Loss: 0.02423200011253357\n",
      "#10 Loss: 0.021906867623329163\n",
      "#11 Loss: 0.019934270530939102\n",
      "#12 Loss: 0.018248530104756355\n",
      "#13 Loss: 0.016798006370663643\n",
      "#14 Loss: 0.015541750937700272\n",
      "#15 Loss: 0.014447162859141827\n",
      "#16 Loss: 0.013488010503351688\n",
      "#17 Loss: 0.012643102556467056\n",
      "#18 Loss: 0.011895153671503067\n",
      "#19 Loss: 0.011229979805648327\n",
      "#20 Loss: 0.010635868646204472\n",
      "#21 Loss: 0.010103111155331135\n",
      "#22 Loss: 0.00962356198579073\n",
      "#23 Loss: 0.009190385229885578\n",
      "#24 Loss: 0.008797802962362766\n",
      "#25 Loss: 0.008440916426479816\n",
      "#26 Loss: 0.00811552070081234\n",
      "#27 Loss: 0.007818029262125492\n",
      "#28 Loss: 0.007545334752649069\n",
      "#29 Loss: 0.007294775918126106\n",
      "#30 Loss: 0.0070640030317008495\n",
      "#31 Loss: 0.006851005833595991\n",
      "#32 Loss: 0.006653991062194109\n",
      "#33 Loss: 0.006471417844295502\n",
      "#34 Loss: 0.0063019003719091415\n",
      "#35 Loss: 0.006144227460026741\n",
      "#36 Loss: 0.0059973313473165035\n",
      "#37 Loss: 0.005860240664333105\n",
      "#38 Loss: 0.005732119083404541\n",
      "#39 Loss: 0.005612190812826157\n",
      "#40 Loss: 0.005499775987118483\n",
      "#41 Loss: 0.00539427949115634\n",
      "#42 Loss: 0.005295128095895052\n",
      "#43 Loss: 0.0052018361166119576\n",
      "#44 Loss: 0.005113945808261633\n",
      "#45 Loss: 0.005031061824411154\n",
      "#46 Loss: 0.0049528032541275024\n",
      "#47 Loss: 0.004878832492977381\n",
      "#48 Loss: 0.004808857571333647\n",
      "#49 Loss: 0.00474258279427886\n",
      "#50 Loss: 0.004679771140217781\n",
      "#51 Loss: 0.004620168823748827\n",
      "#52 Loss: 0.004563577473163605\n",
      "#53 Loss: 0.00450979033485055\n",
      "#54 Loss: 0.004458630923181772\n",
      "#55 Loss: 0.00440992834046483\n",
      "#56 Loss: 0.004363537300378084\n",
      "#57 Loss: 0.004319306463003159\n",
      "#58 Loss: 0.00427711708471179\n",
      "#59 Loss: 0.004236831795424223\n",
      "#60 Loss: 0.004198347683995962\n",
      "#61 Loss: 0.004161564167588949\n",
      "#62 Loss: 0.004126375075429678\n",
      "#63 Loss: 0.004092690069228411\n",
      "#64 Loss: 0.004060433711856604\n",
      "#65 Loss: 0.004029527772217989\n",
      "#66 Loss: 0.0039998870342969894\n",
      "#67 Loss: 0.003971453290432692\n",
      "#68 Loss: 0.003944164142012596\n",
      "#69 Loss: 0.003917956259101629\n",
      "#70 Loss: 0.003892771201208234\n",
      "#71 Loss: 0.0038685575127601624\n",
      "#72 Loss: 0.003845270024612546\n",
      "#73 Loss: 0.0038228591438382864\n",
      "#74 Loss: 0.003801285522058606\n",
      "#75 Loss: 0.003780508413910866\n",
      "#76 Loss: 0.003760487539693713\n",
      "#77 Loss: 0.003741186112165451\n",
      "#78 Loss: 0.0037225699052214622\n",
      "#79 Loss: 0.003704613074660301\n",
      "#80 Loss: 0.0036872795317322016\n",
      "#81 Loss: 0.0036705455277115107\n",
      "#82 Loss: 0.003654371714219451\n",
      "#83 Loss: 0.003638751804828644\n",
      "#84 Loss: 0.003623644122853875\n",
      "#85 Loss: 0.003609041916206479\n",
      "#86 Loss: 0.0035949114244431257\n",
      "#87 Loss: 0.00358123448677361\n",
      "#88 Loss: 0.0035679962020367384\n",
      "#89 Loss: 0.003555173287168145\n",
      "#90 Loss: 0.0035427547991275787\n",
      "#91 Loss: 0.003530713962391019\n",
      "#92 Loss: 0.0035190454218536615\n",
      "#93 Loss: 0.0035077303182333708\n",
      "#94 Loss: 0.003496752353385091\n",
      "#95 Loss: 0.0034861050080507994\n",
      "#96 Loss: 0.0034757619723677635\n",
      "#97 Loss: 0.0034657299984246492\n",
      "#98 Loss: 0.00345598254352808\n",
      "#99 Loss: 0.0034465084318071604\n",
      "Predicted data based on trained weights: \n",
      "Input (scaled): \n",
      "tensor([0.5000, 1.0000])\n",
      "Output: \n",
      "tensor([0.9291])\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "NN = NeuralNetwork()\n",
    "for i in range(100):\n",
    "    #NN(X) automatically calls the forward so there is no need to explicity call forward\n",
    "    print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(X))**2).detach().item()))  # mean sum squared loss\n",
    "    NN.train(X, y)\n",
    "NN.saveWeights(NN)\n",
    "NN.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
